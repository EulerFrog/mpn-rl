% References for MPN-RL Project
% Multi-Plasticity Networks for Reinforcement Learning

% ========================================
% MAIN CITATIONS
% ========================================

@article{aitken_neural_2023,
    title = {Neural population dynamics of computing with synaptic modulations},
    volume = {12},
    issn = {2050-084X},
    url = {https://elifesciences.org/articles/83035},
    doi = {10.7554/eLife.83035},
    abstract = {In addition to long-timescale rewiring, synapses in the brain are subject to significant modulation that occurs at faster timescales that endow the brain with additional means of processing information. Despite this, models of the brain like recurrent neural networks (RNNs) often have their weights frozen after training, relying on an internal state stored in neuronal activity to hold task-relevant information. In this work, we study the computational potential and resulting dynamics of a network that relies solely on synapse modulation during inference to process task-relevant information, the multi-plasticity network (MPN). Since the MPN has no recurrent connections, this allows us to study the computational capabilities and dynamical behavior contributed by synapses modulations alone. The generality of the MPN allows for our results to apply to synaptic modulation mechanisms ranging from short-term synaptic plasticity (STSP) to slower modulations such as spike-time dependent plasticity (STDP). We thoroughly examine the neural population dynamics of the MPN trained on integration-based tasks and compare it to known RNN dynamics, finding the two to have fundamentally different attractor structure. We find said differences in dynamics allow the MPN to outperform its RNN counterparts on several neuroscience-relevant tests. Training the MPN across a battery of neuroscience tasks, we find its computational capabilities in such settings is comparable to networks that compute with recurrent connections. Altogether, we believe this work demonstrates the computational possibilities of computing with synaptic modulations and highlights important motifs of these computations so that they can be identified in brain-like systems.},
    pages = {e83035},
    journaltitle = {eLife},
    author = {Aitken, Kyle and Mihalas, Stefan},
    urldate = {2025-02-01},
    date = {2023-02-23},
    langid = {english},
}

@misc{mnih_playing_2013,
    title = {Playing Atari with Deep Reinforcement Learning},
    url = {http://arxiv.org/abs/1312.5602},
    doi = {10.48550/arXiv.1312.5602},
    abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
    number = {{arXiv}:1312.5602},
    publisher = {{arXiv}},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
    urldate = {2025-02-01},
    date = {2013-12-19},
    eprinttype = {arxiv},
    eprint = {1312.5602},
    keywords = {Computer Science - Machine Learning},
}

@book{sutton_reinforcement_2018,
    title = {Reinforcement Learning: An Introduction},
    isbn = {978-0-262-03924-6},
    edition = {2nd},
    publisher = {MIT Press},
    address = {Cambridge, MA},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    date = {2018},
}
